{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "240654965e339d128a9bbd5be5d7766dbc7de156d15b1c12ed426f21518e5300"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CCTPY CUDA 程序开发笔记\n",
    "\n",
    "## hello-world\n",
    "\n",
    "先用 hello-world 回顾一下利用纯 C 语言和 pychuda 开发 CUDA 程序"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<pre>\n",
    "#include &lt;stdio.h&gt;\n",
    "\n",
    "// 需要从 CPU 端调用，让 GPU 执行的函数称为内核函数，需要用 __global__ 修饰\n",
    "__global__ void kernel_function(){\n",
    "    printf(\"hello, world! -- from gpu\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"hello, world! -- from cpu\\n\");\n",
    "\n",
    "    // 调用内核函数，使用 <<<>>> 分配块和线程\n",
    "    // 一个块内有多个线程，块内支持内存共享\n",
    "    // 块和线程都是三维结构\n",
    "    // 下面定义了 1*1*1 个块，1*1*1 个线程\n",
    "    kernel_function<<<dim3(1,1,1),dim3(1,1,1)>>>();\n",
    "}\n",
    "</pre>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hello, pycuda! -- from python\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n注意：jupyter notebook 中没有打印 GPU 运行信息，可能是监听的 IO 流不对\\n在独立的 py 文件中，能收到 GPU 的打印\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# 内核函数定义\n",
    "mod = SourceModule(\"\"\"\n",
    "    // 需要从 CPU 端调用，让 GPU 执行的函数称为内核函数，需要用 __global__ 修饰\n",
    "    __global__ void kernel_function(){\n",
    "        printf(\"hello, world! -- from gpu\\\\n\");\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# 获取内核函数\n",
    "kernel_function = mod.get_function(\"kernel_function\")\n",
    "\n",
    "# 调用内核函数\n",
    "# grid 就是块数目定义，block 是块内内存\n",
    "kernel_function(grid=(1,1,1), block=(1,1,1))\n",
    "print(\"hello, pycuda! -- from python\")\n",
    "\n",
    "\"\"\"\n",
    "注意：jupyter notebook 中没有打印 GPU 运行信息，可能是监听的 IO 流不对\n",
    "在独立的 py 文件中，能收到 GPU 的打印\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "## 块和线程\n",
    "\n",
    "利用 blockIdx.x 可以访问块 x 方向编号\n",
    "\n",
    "利用 threadIdx.x 可以访问线程 x 方向编号\n",
    "\n",
    "GPU 加速粒子跟踪，我的想法是：一个块负责一个束线的一个粒子"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<pre>\n",
    "#include &lt;stdio.h&gt;\n",
    "\n",
    "// 需要从 CPU 端调用，让 GPU 执行的函数称为内核函数，需要用 __global__ 修饰\n",
    "__global__ void kernel_function(){\n",
    "    int bid = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    printf(\"bid=%d, tid=%d\\n\",bid,tid);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"hello, world! -- from cpu\\n\");\n",
    "\n",
    "    // 调用内核函数，使用 <<<>>> 分配块和线程\n",
    "    // 一个块内有多个线程，块内支持内存共享\n",
    "    // 块和线程都是三维结构\n",
    "    // 下面定义了 1*1*1 个块，1*1*1 个线程\n",
    "\n",
    "    kernel_function<<<dim3(1,1,1),dim3(1,1,1)>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"---------\\n\");\n",
    "    kernel_function<<<dim3(3,1,1),dim3(1,1,1)>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"---------\\n\");\n",
    "    kernel_function<<<dim3(1,1,1),dim3(3,1,1)>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"---------\\n\");\n",
    "    kernel_function<<<dim3(2,1,1),dim3(2,1,1)>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"---------\\n\");\n",
    "}\n",
    "</pre>\n",
    "\n",
    "**程序输出**\n",
    "\n",
    "<pre>\n",
    "hello, world! -- from cpu\n",
    "bid=0, tid=0\n",
    "---------\n",
    "bid=0, tid=0\n",
    "bid=2, tid=0\n",
    "bid=1, tid=0\n",
    "---------\n",
    "bid=0, tid=0\n",
    "bid=0, tid=1\n",
    "bid=0, tid=2\n",
    "---------\n",
    "bid=0, tid=0\n",
    "bid=0, tid=1\n",
    "bid=1, tid=0\n",
    "bid=1, tid=1\n",
    "---------\n",
    "</pre>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# CUDA 简单运算\n",
    "\n",
    "## 单个数加法\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12.]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "\n",
    "# 内核函数定义\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void add(float *a, float *b,float *ret){\n",
    "        *ret = *a + *b;\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "add = mod.get_function(\"add\")\n",
    "\n",
    "a = np.array([10.0], dtype=np.float32)\n",
    "b = np.array([2], dtype=np.float32)\n",
    "c = np.empty(1, dtype=np.float32)\n",
    "\n",
    "add(drv.In(a), drv.In(b), drv.Out(c), grid=(1,1,1), block=(1,1,1))\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "source": [
    "## 向量加法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.18910336 0.10802753 0.8828782  0.19864516 0.10315145]\n[0.48107558 0.2623948  0.770267   0.56897557 0.9377996 ]\n[0.67017895 0.3704223  1.6531452  0.76762074 1.040951  ]\n差异：[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "\n",
    "# 内核函数定义\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void add(float *a, float *b,int *length,float *ret){\n",
    "        int tid = threadIdx.x;\n",
    "        if(tid < *length)\n",
    "            ret[tid] = a[tid] + b[tid];\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "add = mod.get_function(\"add\")\n",
    "\n",
    "length = 5\n",
    "a = np.random.rand(length).astype(np.float32)\n",
    "b = np.random.rand(length).astype(np.float32)\n",
    "c = np.empty_like(a)\n",
    "\n",
    "add(drv.In(a), drv.In(b),drv.In(np.array([length],dtype=np.int32)), drv.Out(c), grid=(1,1,1), block=(length,1,1))\n",
    "\n",
    "print(a,b,c,sep='\\n')\n",
    "print(f\"差异：{c-a-b}\")"
   ]
  },
  {
   "source": [
    "# 规约运算（加法规约 == 求和）\n",
    "\n",
    "第一步：把 block 内数据求和（两两分组求和）\n",
    "\n",
    "### 问题 1 如何减少线程束内的分束？使用图中第二种规约方法\n",
    "\n",
    "<img src=\"./img/C01规约运算减少线程束内的份束.jpg\"></img>\n",
    "\n",
    "### 问题 2 数据存放在哪？全局内存？共享内存？是否需要同步？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "原数组[ 2 10 62 ... 32 97 47]\nCUDA=[51746 50427 50581 ... 49424 50193 51345], CPU=[51746 50427 50581 ... 49424 50193 51345]\n计算正确\nCPU用时0.011775016784667969秒, GPU用时0.0263516902923584秒\n"
     ]
    }
   ],
   "source": [
    "# block 内规约求和\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 内核函数定义\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void block_summer(int* array,int* block_sum){\n",
    "        int local_id = threadIdx.x; // block 内 tid\n",
    "        int global_id = blockDim.x * blockIdx.x + local_id; // 全局 tid\n",
    "        __shared__ int s[1024]; // block 内共享内存\n",
    "\n",
    "        s[local_id] = array[global_id]; // 复制到共享内存\n",
    "        __syncthreads(); // 块内同步\n",
    "\n",
    "        for(int step = blockDim.x>>1; step >= 1; step>>=1 ){ // 跨步初始化为 block 大小的一半，并且每次减半\n",
    "            if(local_id < step) s[local_id] += s[local_id+step]; // 跨步规约\n",
    "        }\n",
    "\n",
    "        if(local_id == 0) block_sum[blockIdx.x] = s[0]; // 每个块规约结束，则局部和放在了 s[0] 中\n",
    "        // 这一步不用同步，应为最后一次加法执行人，就是 local_id == 0 \n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "block_summer = mod.get_function(\"block_summer\")\n",
    "\n",
    "block_size = 1024\n",
    "grid_size = 24000\n",
    "\n",
    "array = np.random.randint(100,size=block_size*grid_size)\n",
    "block_sum = np.empty((grid_size,),dtype=np.int32)\n",
    "\n",
    "s1 = time.time()\n",
    "block_summer(drv.In(array), drv.Out(block_sum), grid=(grid_size,1,1), block=(block_size,1,1))\n",
    "\n",
    "s2 = time.time()\n",
    "block_sum_cpu = np.sum(array.reshape(grid_size,block_size),axis=1)\n",
    "e = time.time()\n",
    "\n",
    "print(f\"原数组{array}\")\n",
    "print(f\"CUDA={block_sum}, CPU={block_sum_cpu}\")\n",
    "if np.sum(np.abs(block_sum-block_sum_cpu))==0:\n",
    "    print(\"计算正确\")\n",
    "print(f\"CPU用时{e-s2}秒, GPU用时{s2-s1}秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "原数组[73 19 10 ... 66  6  1]\nCUDA=[1216424266], CPU=1216424266\n计算正确\nCPU用时0.00976109504699707秒, GPU用时0.023424148559570312秒\n"
     ]
    }
   ],
   "source": [
    "# 全局求和，和上代码不同的仅仅是把  if(local_id == 0) block_sum[blockIdx.x] = s[0]; \n",
    "# 改成 if(local_id == 0) atomicAdd(global_sum, s[0]);\n",
    "\n",
    "# block 内规约求和\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 内核函数定义\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void sum(int* array,int* global_sum){\n",
    "        int local_id = threadIdx.x; // block 内 tid\n",
    "        int global_id = blockDim.x * blockIdx.x + local_id; // 全局 tid\n",
    "        __shared__ int s[1024]; // block 内共享内存\n",
    "\n",
    "        s[local_id] = array[global_id]; // 复制到共享内存\n",
    "        __syncthreads(); // 块内同步\n",
    "\n",
    "        for(int step = blockDim.x>>1; step >= 1; step>>=1 ){ // 跨步初始化为 block 大小的一半，并且每次减半\n",
    "            if(local_id < step) s[local_id] += s[local_id+step]; // 跨步规约\n",
    "        }\n",
    "\n",
    "        if(local_id == 0) atomicAdd(global_sum, s[0]);\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "sum = mod.get_function(\"sum\")\n",
    "\n",
    "block_size = 1024\n",
    "grid_size = 24000\n",
    "\n",
    "array = np.random.randint(100,size=block_size*grid_size)\n",
    "global_sum = np.zeros((1,),dtype=np.int32)\n",
    "\n",
    "s1 = time.time()\n",
    "sum(drv.In(array), drv.Out(global_sum), grid=(grid_size,1,1), block=(block_size,1,1))\n",
    "\n",
    "s2 = time.time()\n",
    "global_sum_cpu = np.sum(array)\n",
    "e = time.time()\n",
    "\n",
    "print(f\"原数组{array}\")\n",
    "print(f\"CUDA={global_sum}, CPU={global_sum_cpu}\")\n",
    "if np.sum(np.abs(block_sum-block_sum_cpu))==0:\n",
    "    print(\"计算正确\")\n",
    "print(f\"CPU用时{e-s2}秒, GPU用时{s2-s1}秒\")\n"
   ]
  },
  {
   "source": [
    "# 重要知识点\n",
    "\n",
    "- __syncthreads() block 内线程同步，一般用于对 __share__ 变量的写读之间\n",
    "\n",
    "- 全一维下线程全局 id = blockDim.x + blockIdx.x + threadIdx.x\n",
    "\n",
    "- __shared__ 表示 block 内的线程共享变量\n",
    "\n",
    "- 原子操作 atomic add / sub / ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}